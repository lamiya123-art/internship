# -*- coding: utf-8 -*-
"""Internship 1 assignment 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o4gxmfXh4UMrDe_ARdCN6_PHuqxVg_l6
"""

import pandas as pd
import numpy as np

import os
print(os.getcwd())

original_file =r"C:\Users\Lenovo\OneDrive\Downloads\climate_change_download_0.xls"

from google.colab import files
uploaded = files.upload()

data_original = pd.read_excel("climate_change_download_0.xls")

"""Global Data Overview"""

print("shape of the original data set")
data_original.shape

print("Available colums:")
data_original.columns

print("Column data types:")
data_original.dtypes

print("overview of the first 5 rows")
data_original.head()

print("Descriptive understanding:")
data_original.describe()

"""print the contents of 'series name'"""

data_original['Series name'].unique()

"""display contents of the column series code"""

data_original['Series code'].unique()

"""display column scale"""

data_original['SCALE'].unique()

"""display decimals"""

data_original['Decimals'].unique()

"""under scale and decimals columns"""

data_original[data_original['SCALE']=='Text']

data_original[data_original['Decimals']=='Text']

"""Removing rows marked as"text" in the "Scale" and "Decimals" columns"""

data_clean = data_original
print("original number of rows:")
print(data_clean.shape[0])

data_clean =data_clean[data_clean['SCALE']!='Text']

print("Current number of rows:")
print(data_clean.shape[0])

""""Country name" "series code" "scale" "decimals"are useless. remove them."""

print("Original number of columns:")
print(data_clean.shape[1])
data_clean = data_clean.drop(['Country name', 'Series code', 'SCALE', 'Decimals'],axis='columns')
print("Current number of columns:")
print(data_clean.shape[1])

"""transform the ".." strings and empty ("") into NaN values FOR EASIER Recognission as missing values"""

data_clean.iloc[:,2:] = data_clean.iloc[:,2:].replace({'':np.nan,'..':np.nan})

"""transform all data columns into numerical data type"""

data_clean2 = data_clean.applymap(lambda x: pd.to_numeric(x,errors='ignore'))
print("print")
data_clean2.dtypes

"""Rename the features in column "series name"
"""

chosen_vars = {'Cereal yield (kg per hectare)': 'cereal_yield',
               'Foreign direct investment, net inflows (% of GDP)': 'fdi_perc_gdp',
               'Access to electricity (% of total population)': 'elec_access_perc',
               'Energy use per units of GDP (kg oil eq./$1,000 of 2005 PPP $)': 'en_per_gdp',
               'Energy use per capita (kilograms of oil equivalent)': 'en_per_cap',
               'CO2 emissions, total (KtCO2)': 'co2_ttl',
               'CO2 emissions per capita (metric tons)': 'co2_per_cap',
               'CO2 emissions per units of GDP (kg/$1,000 of 2005 PPP $)': 'co2_per_gdp',
               'Other GHG emissions, total (KtCO2e)': 'other_ghg_ttl',
               'Methane (CH4) emissions, total (KtCO2e)': 'ch4_ttl',
               'Nitrous oxide (N2O) emissions, total (KtCO2e)': 'n2o_ttl',
               'Droughts, floods, extreme temps (% pop. avg. 1990-2009)': 'nat_emerg',
               'Population in urban agglomerations >1million (%)': 'pop_urb_aggl_perc',
               'Nationally terrestrial protected areas (% of total land area)': 'prot_area_perc',
               'GDP ($)': 'gdp',
               'GNI per capita (Atlas $)': 'gni_per_cap',
               'Under-five mortality rate (per 1,000)': 'under_5_mort_rate',
               'Population growth (annual %)': 'pop_growth_perc',
               'Population': 'pop',
               'Urban population growth (annual %)': 'urb_pop_growth_perc',
               'Urban population': 'urb_pop'
                }

# rename all variables in the column "Series name" with comprehensible shorter versions
data_clean2['Series name'] = data_clean2['Series name'].replace(to_replace=chosen_vars)

"""data frame transformation"""

data_clean2.head()

#save the short feature names into a list of strings
chosen_cols =list(chosen_vars.values())
#define an empty list, where sub dataframes for each feature will be saved
frame_list =[]
#iterate over all chosen features
for variable in chosen_cols:
  #pick the rows coresponding to the current feature
  frame =data_clean2[data_clean2['Series name']==variable]
  #melt all the values for all years into one column and rename the columns correspondingly
  frame = frame.melt(id_vars=['Country code', 'Series name']).rename(columns={'Country code': 'country', 'variable': 'year', 'value': variable}).drop(['Series name'], axis='columns')
  #add the melted dataframe for the current feature into the list
  frame_list.append(frame)

  #merge all sub frames into a single dataframe, making an outer binding on the key columns
  from functools import reduce
  all_vars =reduce(lambda left,right:pd.merge(left,right,on=['country','year'],how='outer'),frame_list)

all_vars.head()

"""remove the missing values"""

print("check the amount of missing values")
all_vars.isnull().sum()

"""Filtering the years by missing values"""

all_vars_clean = all_vars

#define an array with the unique year values
years_count_missing = dict.fromkeys(all_vars_clean['year'].unique(), 0)
for ind, row in all_vars_clean.iterrows():
    years_count_missing[row['year']] += row.isnull().sum()

# sort the years by missing values
years_missing_sorted = dict(sorted(years_count_missing.items(), key=lambda item: item[1]))

# print the missing values for each year
print("missing values by year:")
for key, val in years_missing_sorted.items():
    print(key, ":", val)

"""main goal is to  remove countries with excessive missing data while preserving the time span as much as possible."""

print("number of missing values before filtering")
print(all_vars_clean.isnull().sum().sum())
print("number of rows before filtering")
print(all_vars_clean.shape[0])

all_vars_clean =all_vars_clean[(all_vars_clean['year']>= 1991)&(all_vars_clean['year']<= 2000)]

print("number of missing values in the whole dataset after filtering the years:")
print(all_vars_clean.isnull().sum().sum())
print("number of rows after filtering the years:")
print(all_vars_clean.shape[0])

"""Filtering countries by missing values"""

#define an array with the unique country values
countries_count_missing = dict.fromkeys(all_vars_clean['country'].unique(),0)

#iterate through all columns and rows count the amount of NaN values for each country
# iterate through all rows and count the amount of NaN values for each country
for ind, row in all_vars_clean.iterrows():
    countries_count_missing[row['country']] += row.isnull().sum()

# sort the countries by missing values
countries_missing_sorted = dict(sorted(countries_count_missing.items(), key=lambda item: item[1]))

# print the missing values for each country
print("missing values by country:")
for key, val in countries_missing_sorted.items():
    print(key, ":", val)

print("number of missing values in the whole dataset before filtering the countries:")
print(all_vars_clean.isnull().sum().sum())
print("number of rows before filtering the countries:")
print(all_vars_clean.shape[0])


# filter only rows for countries with less than 90 missing values
countries_filter = []
for key, val in countries_missing_sorted.items():
    if val<90:
        countries_filter.append(key)

all_vars_clean = all_vars_clean[all_vars_clean['country'].isin(countries_filter)]

print("number of missing values in the whole dataset after filtering the countries:")
print(all_vars_clean.isnull().sum().sum())
print("number of rows after filtering the countries:")
print(all_vars_clean.shape[0])

all_vars_clean.isnull().sum()

"""dropping high NaN features"""

from itertools import compress

# ðŸ“Š Adjust threshold to keep more COâ‚‚-related predictors
missing_threshold = 50

# ðŸ‘€ Show missing values per column before dropping
print("Missing values before column filtering:")
print(all_vars_clean.isnull().sum())

# Drop only columns with more than `missing_threshold` missing values
vars_bad = all_vars_clean.isnull().sum() > missing_threshold
all_vars_clean2 = all_vars_clean.drop(
    compress(data=all_vars_clean.columns, selectors=vars_bad),
    axis='columns'
)

print("\nColumns retained (important for COâ‚‚ prediction):")
print(all_vars_clean2.columns)

# Final clean-up: drop rows with any remaining missing values
all_vars_clean3 = all_vars_clean2.dropna(axis='rows', how='any')

print("\nRemaining missing values per column:")
print(all_vars_clean3.isnull().sum())

print("\nFinal shape of the cleaned dataset:")
print(all_vars_clean3.shape)

# delete rows with any number of missing values
all_vars_clean3 = all_vars_clean2.dropna(axis='rows', how='any')

print("Remaining missing values per column:")
print(all_vars_clean3.isnull().sum())

print("Final shape of the cleaned dataset:")
print(all_vars_clean3.shape)

# export the clean dataframe to a csv file
all_vars_clean3.to_csv('data_cleaned.csv', index=False)

from google.colab import files
files.download('data_cleaned.csv')